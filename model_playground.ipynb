{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Playground\n",
    "This file serves as an easy, low effort platform to get a feel for how models behave before and after my fine-tuning runs. With extra comforts like interactive inputs and real-time streaming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick prep\n",
    "This is for fast downloads!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q huggingface_hub[hf_transfer]\n",
    "# !export HF_HUB_ENABLE_HF_TRANSFER=1\n",
    "# !huggingface-cli download Neelectric/OLMo-2-1124-7B-Instruct_SFTv00.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/repos/ModelDiffing/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model registry\n",
    "Here we choose the model(s) and corresponding tokenizers, as well as datasets we want to load in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id_base = \"allenai/OLMo-2-1124-7B-Instruct\"\n",
    "model_id_ft = \"Neelectric/OLMo-2-1124-7B-Instruct_SFTv00.05\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_base = AutoTokenizer.from_pretrained(model_id_base)\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(model_id_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.90it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "device_base = \"cuda:0\"\n",
    "device_ft = \"cuda:1\"\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_base,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_base    \n",
    ").to(device_base)\n",
    "model_ft = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_ft,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_ft  \n",
    ").to(device_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"open-r1/OpenR1-Math-220k\"\n",
    "split_name = \"train\"\n",
    "open_r1 = load_dataset(dataset_name)[split_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_base = TextStreamer(tokenizer_base)\n",
    "streamer_ft = TextStreamer(tokenizer_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prompt for some user input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (alternatively, override with this open-r1/OpenR1-Math-220k question (first one in the dataset)\n",
    "user_input = \"\"\"## Task B-1.3.\n",
    "\n",
    "A ship traveling along a river has covered $24 \\mathrm{~km}$ upstream and $28 \\mathrm{~km}$ downstream. For this journey, it took half an hour less than for traveling $30 \\mathrm{~km}$ upstream and $21 \\mathrm{~km}$ downstream, or half an hour more than for traveling $15 \\mathrm{~km}$ upstream and $42 \\mathrm{~km}$ downstream, assuming that both the ship and the river move uniformly.\n",
    "\n",
    "Determine the speed of the ship in still water and the speed of the river.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input()\n",
    "chatML_input = [{\"role\":\"user\", \"content\": user_input}]\n",
    "print(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model\n",
    "Time to see what the base model says here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "templated_base = tokenizer_base.apply_chat_template(\n",
    "        chatML_input, \n",
    "        tokenize=False, \n",
    "        truncation=False,\n",
    "        # return_tensors=\"pt\",\n",
    "        add_generation_prompt=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40])\n",
      "<|endoftext|><|user|>\n",
      "When I apply supervised fine-tuning, should special tokens from the tokenizer be masked out to a label of -100 so pytorch ignores them?\n",
      "<|assistant|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you apply supervised fine-tuning, especially in the context of natural language processing (NLP) tasks using models like BERT, GPT, or RoBERTa, it's common to mask out certain tokens to force the model to learn meaningful representations of the input text. These masked tokens are typically replaced with a special token (often `[MASK]` or `<MASK>`) to indicate that the model should predict their value based on the context provided by the other unmasked tokens.\n",
      "\n",
      "However, using a label of `-100` or any other negative number to mask tokens is unconventional and not recommended for the following reasons:\n",
      "\n",
      "1. **Model Interpretability**: Using a standard token like `[MASK]` or `<MASK>` is widely recognized and easily interpretable by others working with your model. Negative numbers, especially `-100`, do not convey this meaning clearly and could lead to confusion.\n",
      "\n",
      "2. **Training Stability**: Negative numbers might not be handled correctly by the model during training, potentially causing numerical instability or incorrect gradients, especially if the model's internal representations or operations are not designed to handle such values gracefully.\n",
      "\n",
      "3. **Tokenization Compatibility**: Most NLP tokenizers are designed to handle a finite set of tokens (e.g., ASCII characters, a set of subwords, or Unicode characters). Using a negative number as a token would not be compatible with these tokenizers and could cause errors during tokenization.\n",
      "\n",
      "4. **Consistency Across Models**: Using a standard token like `[MASK]` ensures consistency across different models and datasets. This standardization makes it easier to understand, replicate, and extend the fine-tuning process.\n",
      "\n",
      "Instead of using `-100` or any other negative number, stick to the conventional approach of using `[MASK]` or `<MASK>` for masked tokens. If you're working with a tokenizer that supports additional special tokens (like `[PAD]`, `[SEP]`, etc.), you can also use those to enhance the model's ability to understand and predict the masked tokens correctly.\n",
      "\n",
      "In summary, while the idea of using a unique token like `-100` might seem appealing for distinguishing between different types of masked tokens, it's better to adhere to the established conventions in the NLP community to ensure compatibility, stability, and clarity in your fine-tuning process.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "inputs_base = tokenizer_base(templated_base, return_tensors=\"pt\").to(device_base)\n",
    "print(inputs_base.input_ids.shape)\n",
    "\n",
    "output_base = model_base.generate(\n",
    "    **inputs_base, \n",
    "    streamer=streamer_base, \n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned model\n",
    "And now we check what the model thinks after my fine-tuning was applied!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "templated_ft = tokenizer_ft.apply_chat_template(\n",
    "        chatML_input, \n",
    "        tokenize=False, \n",
    "        truncation=False,\n",
    "        # return_tensors=\"pt\",\n",
    "        add_generation_prompt=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 102])\n",
      "<|endoftext|><|system|>\n",
      "You are a helpful AI Assistant that provides well-reasoned and detailed responses. You first think about the reasoning process as an internal monologue and then provide the user with the answer. Respond in the following format: <think>\n",
      "...\n",
      "</think>\n",
      "<answer>\n",
      "...\n",
      "</answer>\n",
      "<|user|>When I apply supervised fine-tuning, should special tokens from the tokenizer be masked out to a label of -100 so pytorch ignores them?\n",
      "<|assistant|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out whether special tokens from the tokenizer should be masked out to a label of -100 when applying supervised fine-tuning. Hmm, let me start by recalling what supervised fine-tuning is. From what I remember, it's a technique used in machine learning where you take a pre-trained model and fine-tune it on a specific task. The idea is to leverage the existing knowledge in the pre-trained model while adjusting it to the new task. \n",
      "\n",
      "Now, when you fine-tune a model, you usually do this by training it on a dataset that's relevant to the task you want to solve. The model learns from the data, updating its weights to perform better on the new task. But here's the thing: during this process, the model might encounter tokens that it hasn't seen before. These could be special tokens, like the ones added by the tokenizer to represent things like padding, start, and end of sequences, or maybe even other tokens that are specific to the tokenizer's implementation.\n",
      "\n",
      "The question is whether these special tokens should be masked out during training. The user mentioned that they should be replaced with a label of -100, and the PyTorch model should ignore them. So, why is that necessary? Let me think.\n",
      "\n",
      "First, when you fine-tune a model, you want it to focus on learning the patterns and relationships in the data that are relevant to the task. If the model encounters tokens that it hasn't been trained on, it might not know how to handle them. For example, if the tokenizer uses a special token to represent the end of a sequence, the model might not understand the semantic meaning of that token. If the model tries to assign a value to that token during training, it might learn the wrong thing, like associating that token with a particular class or feature. This could lead to the model making incorrect predictions or overfitting to the special tokens instead of the actual data.\n",
      "\n",
      "To prevent this, the standard practice is to mask out these special tokens. By replacing them with a label like -100, which is a value that the model hasn't been trained on, the model is forced to ignore those tokens during training. This is similar to how dropout is used in neural networks to prevent overfitting by randomly setting some neurons to zero during training. In this case, masking out the special tokens effectively \"drops out\" their contribution to the model's predictions, forcing the model to learn the underlying data patterns without being influenced by the special tokens.\n",
      "\n",
      "Moreover, if the special tokens are not masked out, the model might assign them some arbitrary value during training. For example, if the tokenizer uses a special token to represent the start of a sequence, the model might learn that this token is associated with a particular class or feature. But in reality, that token is just a placeholder and doesn't carry any meaningful information. If the model starts to rely on these tokens for making predictions, it might perform poorly on unseen data because those tokens might not be present there. This is known as \"leaking\" information from the training data into the model, which can lead to overfitting.\n",
      "\n",
      "Another consideration is that the tokenizer might have different versions or configurations, and the special tokens could vary between them. If the model is trained on one tokenizer and then used with another, the special tokens might not match, leading to confusion. By masking out all special tokens, the model is trained to ignore them regardless of the tokenizer's specifics, which makes the model more robust and transferable across different tokenizer configurations.\n",
      "\n",
      "Additionally, in some cases, the special tokens might be part of the actual data. For example, if the dataset includes text data that uses certain tokens to represent specific concepts, the model might confuse those tokens with the special tokens used by the tokenizer. Masking them out ensures that the model doesn't learn any associations with those tokens, thereby avoiding confusion.\n",
      "\n",
      "So, in summary, masking out special tokens during supervised fine-tuning is a good practice to prevent the model from learning arbitrary associations with those tokens, to ensure that the model focuses on the actual data, and to make the model more robust and transferable across different tokenizer configurations. By replacing them with a label like -100, which the model hasn't been trained on, we effectively tell the model to ignore those tokens, forcing it to learn the underlying patterns in the data.\n",
      "</think>\n",
      "\n",
      "When applying supervised fine-tuning, special tokens from the tokenizer should be masked out to a label of -100. This is because:\n",
      "\n",
      "1. **Preventing Overfitting**: Special tokens do not carry meaningful information and can lead the model to overfit to these placeholders instead of the actual data. Masking them forces the model to ignore these tokens during training.\n",
      "2. **Robustness**: Masking ensures the model is not influenced by specific tokenizer configurations, making it more transferable across different tokenizer versions.\n",
      "3. **Avoiding Data Leakage**: Special tokens might be part of the actual data, leading the model to confuse them with meaningful tokens. Masking prevents this confusion.\n",
      "4. **Model Focus**: By ignoring special tokens, the model learns the underlying data patterns more effectively.\n",
      "\n",
      "Replacing special tokens with -100 (a value not seen during training) enforces the model to ignore them, focusing on the task at hand.\n",
      "\n",
      "**Answer**\n",
      "Yes, special tokens should be masked out to a label of -100. This prevents the model from learning arbitrary associations with these tokens, ensuring it focuses on the actual data and remains robust across different tokenizer configurations. The model will ignore these tokens during training, preventing overfitting and data leakage.  \n",
      "\\boxed{Yes}  \n",
      "**Explanation**: Masking special tokens with a label like -100 ensures the model ignores them, focusing on the task and avoiding overfitting to arbitrary placeholders. This practice enhances model robustness and transferability across different tokenizer versions.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "inputs_ft = tokenizer_ft(templated_ft, return_tensors=\"pt\").to(device_ft)\n",
    "print(inputs_ft.input_ids.shape)\n",
    "\n",
    "output_ft = model_ft.generate(\n",
    "    **inputs_ft, \n",
    "    streamer=streamer_ft, \n",
    "    max_new_tokens=4096,\n",
    "    do_sample=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
